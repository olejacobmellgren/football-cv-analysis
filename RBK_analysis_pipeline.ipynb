{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77bb8a5",
   "metadata": {},
   "source": [
    "\n",
    "# RBK Football2025 â€” YOLO Setup (Merged Multi-Match)\n",
    "\n",
    "This notebook is tailored for the dataset structure you described:\n",
    "\n",
    "```\n",
    "Football2025/\n",
    "  RBK-AALESUND/\n",
    "    aalesund_cvat_overlay.mp4\n",
    "    aalesund.mp4\n",
    "    annotations.xml\n",
    "    data/\n",
    "      images/train/*.png\n",
    "    labels/\n",
    "      train/*.txt\n",
    "    data.yaml\n",
    "    train.txt\n",
    "  RBK-BODO/\n",
    "  RBK-FREDRIKSTAD/\n",
    "  RBK-HamKam/\n",
    "  RBK-VIKING/\n",
    "```\n",
    "\n",
    "It will:\n",
    "1. Auto-discover the `RBK-*` folders under `Football2025`.\n",
    "2. Collect images from `data/images/train` and matched labels from `labels/train`.\n",
    "3. Create a **merged YOLO dataset** with `images/train`, `images/val`, `labels/train`, `labels/val` using **symlinks** (no copying).\n",
    "4. Write a `data.yaml` pointing to these merged splits.\n",
    "5. Provide training/eval cells for Ultralytics YOLO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2df04b",
   "metadata": {},
   "source": [
    "## 0) Environment check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4420b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.1\n",
      "CUDA available: False\n",
      "Ultralytics not available yet: dlopen(/Users/olejacobmellgren/football_env/lib/python3.12/site-packages/cv2/cv2.abi3.so, 0x0002): Library not loaded: @loader_path/libaom.3.12.1.dylib\n",
      "  Referenced from: <3AE50086-BBA4-3A54-88C5-5C602E06B65F> /Users/olejacobmellgren/football_env/lib/python3.12/site-packages/cv2/.dylibs/libavif.16.3.0.dylib\n",
      "  Reason: tried: '/Users/olejacobmellgren/football_env/lib/python3.12/site-packages/cv2/.dylibs/libaom.3.12.1.dylib' (no such file), '/usr/lib/libaom.3.12.1.dylib' (no such file, not in dyld cache)Library not loaded: @loader_path/libaom.3.12.1.dylib\n",
      "  Referenced from: <B2C96617-93FD-3626-B71B-B0C15D095AE8> /Users/olejacobmellgren/football_env/lib/python3.12/site-packages/cv2/.dylibs/libavformat.61.7.100.dylib\n",
      "  Reason: tried: '/Users/olejacobmellgren/football_env/lib/python3.12/site-packages/cv2/.dylibs/libaom.3.12.1.dylib' (no such file), '/usr/lib/libaom.3.12.1.dylib' (no such file, not in dyld cache)Library not loaded: @loader_path/libaom.3.12.1.dylib\n",
      "  Referenced from: <F29618F1-053E-3730-A760-1537F3B0C4F4> /Users/olejacobmellgren/football_env/lib/python3.12/site-packages/cv2/.dylibs/libavcodec.61.19.101.dylib\n",
      "  Reason: tried: '/Users/olejacobmellgren/football_env/lib/python3.12/site-packages/cv2/.dylibs/libaom.3.12.1.dylib' (no such file), '/usr/lib/libaom.3.12.1.dylib' (no such file, not in dyld cache)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %pip install --upgrade ultralytics opencv-python pandas pyyaml matplotlib\n",
    "\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Torch/Ultralytics check (optional)\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not available yet:\", e)\n",
    "\n",
    "try:\n",
    "    import ultralytics\n",
    "    from ultralytics import YOLO\n",
    "    print(\"Ultralytics:\", ultralytics.__version__)\n",
    "except Exception as e:\n",
    "    print(\"Ultralytics not available yet:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08c2e4",
   "metadata": {},
   "source": [
    "## 1) Paths & discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da507e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR: /Users/olejacobmellgren/Documents/football_analysis/football_yolo_workdir\n",
      "MERGED_ROOT: /Users/olejacobmellgren/Documents/football_analysis/football_yolo_workdir/merged_yolo\n",
      "DATA_YAML_PATH: /Users/olejacobmellgren/Documents/football_analysis/football_yolo_workdir/football_merged_data.yaml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random, numpy as np\n",
    "\n",
    "# POINT STRAIGHT TO YOUR LOCAL WORKDIR\n",
    "WORK_DIR = (Path.home() / \"Documents\" / \"football_analysis\" / \"football_yolo_workdir\").resolve()\n",
    "MERGED_ROOT = WORK_DIR / \"merged_yolo\"\n",
    "DATA_YAML_PATH = WORK_DIR / \"football_merged_data.yaml\"\n",
    "\n",
    "print(\"WORK_DIR:\", WORK_DIR)\n",
    "print(\"MERGED_ROOT:\", MERGED_ROOT)\n",
    "print(\"DATA_YAML_PATH:\", DATA_YAML_PATH)\n",
    "\n",
    "NAMES = ['player', 'referee', 'ball']\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# optional seeding for torch\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d05c0",
   "metadata": {},
   "source": [
    "## 2) Merge per-match folders into a single YOLO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcc2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN THIS CELL ONCE TO BUILD MERGED DATASET\n",
    "\n",
    "import os, re, shutil, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "def has_yolo_unit_dir(d: Path) -> bool:\n",
    "    \"\"\"Return True if d contains data/images/train and labels/train.\"\"\"\n",
    "    return (d / 'data' / 'images' / 'train').exists() and (d / 'labels' / 'train').exists()\n",
    "\n",
    "def find_match_roots(root: Path) -> List[Path]:\n",
    "    \"\"\"Top-level RBK-* folders.\"\"\"\n",
    "    return [p for p in sorted(root.iterdir()) if p.is_dir() and p.name.startswith('RBK-')]\n",
    "\n",
    "def find_dataset_units(root: Path) -> List[Tuple[Path, str]]:\n",
    "    \"\"\"\n",
    "    Find all usable dataset 'units'.\n",
    "    Each unit is a directory that itself contains data/images/train and labels/train.\n",
    "    Returns list of (unit_dir, unit_tag) where unit_tag is used to keep paths unique.\n",
    "    \"\"\"\n",
    "    units: List[Tuple[Path, str]] = []\n",
    "    for mdir in find_match_roots(root):\n",
    "        # Case A: the RBK-* folder itself is a unit\n",
    "        if has_yolo_unit_dir(mdir):\n",
    "            units.append((mdir, mdir.name))\n",
    "        \n",
    "        # Case B: nested units (e.g., RBK-BODO/part1/RBK_BODO_PART1)\n",
    "        # Search up to a reasonable depth\n",
    "        for sub in mdir.rglob('*'):\n",
    "            if sub.is_dir() and has_yolo_unit_dir(sub):\n",
    "                # Tag keeps RBK-* and the unit dir name to avoid name collisions\n",
    "                tag = f\"{mdir.name}/{sub.name}\"\n",
    "                # Avoid duplicating if we already added mdir itself\n",
    "                if (sub, tag) not in units:\n",
    "                    units.append((sub, tag))\n",
    "    # Remove duplicates (if any)\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for u, tag in units:\n",
    "        key = (u.resolve(), tag)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append((u, tag))\n",
    "    return uniq\n",
    "\n",
    "def pair_images_labels(img_dir: Path, lbl_dir: Path) -> List[Tuple[Path, Path]]:\n",
    "    pairs = []\n",
    "    exts = ('.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG')\n",
    "    for img_path in sorted(img_dir.rglob('*')):\n",
    "        if img_path.is_file() and img_path.suffix in exts:\n",
    "            rel = img_path.relative_to(img_dir)  # preserve any subfolders\n",
    "            lbl_candidate = lbl_dir / rel.with_suffix('.txt')\n",
    "            if lbl_candidate.exists():\n",
    "                pairs.append((img_path, lbl_candidate))\n",
    "    return pairs\n",
    "\n",
    "def symlink(src: Path, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dst.exists() or dst.is_symlink():\n",
    "        dst.unlink()\n",
    "    os.symlink(src, dst)\n",
    "\n",
    "# --- Discover all dataset units (handles RBK-BODO/part{1,2,3}/RBK_BODO_PART*) ---\n",
    "# dataset_units = find_dataset_units(DATA_ROOT)\n",
    "# print(\"Discovered units:\")\n",
    "# for udir, tag in dataset_units:\n",
    "#     print(\"  \", tag, \"->\", udir)\n",
    "\n",
    "# # --- Build (img, lbl, tag) triples across all units ---\n",
    "# all_triples = []\n",
    "# for unit_dir, tag in dataset_units:\n",
    "#     img_dir = unit_dir / 'data' / 'images' / 'train'\n",
    "#     lbl_dir = unit_dir / 'labels' / 'train'\n",
    "#     pairs = pair_images_labels(img_dir, lbl_dir)\n",
    "#     print(f\"{tag}: {len(pairs)} pairs\")\n",
    "#     # keep tag so we can build unique relative names\n",
    "#     all_triples.extend([(ip, lp, tag) for (ip, lp) in pairs])\n",
    "\n",
    "# print(\"Total pairs:\", len(all_triples))\n",
    "\n",
    "# # --- Global 90/10 split ---\n",
    "# random.seed(42)\n",
    "# random.shuffle(all_triples)\n",
    "# n_total = len(all_triples)\n",
    "# n_val = max(1, int(0.1 * n_total))\n",
    "# val_triples = all_triples[:n_val]\n",
    "# train_triples = all_triples[n_val:]\n",
    "\n",
    "# # --- Prepare merged dirs ---\n",
    "# img_train = MERGED_ROOT / 'images' / 'train'\n",
    "# img_val   = MERGED_ROOT / 'images' / 'val'\n",
    "# lbl_train = MERGED_ROOT / 'labels' / 'train'\n",
    "# lbl_val   = MERGED_ROOT / 'labels' / 'val'\n",
    "# for d in [img_train, img_val, lbl_train, lbl_val]:\n",
    "#     d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def rel_name_from_triple(img_path: Path, unit_dir: Path, unit_tag: str) -> str:\n",
    "    \"\"\"\n",
    "    Produce a unique relative path for the merged dataset.\n",
    "    We keep the unit_tag (e.g., 'RBK-BODO/RBK_BODO_PART1') and the relative path under images/train.\n",
    "    \"\"\"\n",
    "    rel_img = img_path.relative_to(unit_dir / 'data' / 'images' / 'train')\n",
    "    return f\"{unit_tag}/{rel_img.as_posix()}\"\n",
    "\n",
    "# --- Symlink into merged dataset ---\n",
    "def link_split(triples, split_name: str):\n",
    "    for img_path, lbl_path, tag in triples:\n",
    "        # Recover the unit_dir for this triple (by reverse lookup)\n",
    "        # Faster approach: pass unit_dir along in triples, but we can reconstruct by tag match:\n",
    "        unit_dir = None\n",
    "        for udir, utag in dataset_units:\n",
    "            if utag == tag:\n",
    "                unit_dir = udir\n",
    "                break\n",
    "        if unit_dir is None:\n",
    "            # Fallback: put under tag flat\n",
    "            rel_name = f\"{tag}/{img_path.name}\"\n",
    "        else:\n",
    "            rel_name = rel_name_from_triple(img_path, unit_dir, tag)\n",
    "\n",
    "        dst_img = MERGED_ROOT / 'images' / split_name / rel_name\n",
    "        dst_lbl = MERGED_ROOT / 'labels' / split_name / Path(rel_name).with_suffix('.txt')\n",
    "        symlink(img_path, dst_img)\n",
    "        symlink(lbl_path, dst_lbl)\n",
    "\n",
    "# link_split(train_triples, 'train')\n",
    "# link_split(val_triples, 'val')\n",
    "\n",
    "# print(f\"Train pairs: {len(train_triples)} | Val pairs: {len(val_triples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c6dc64",
   "metadata": {},
   "source": [
    "## 3) Write `data.yaml` for Ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "330dd2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/olejacobmellgren/Documents/football_analysis/football_yolo_workdir/football_merged_data.yaml\n",
      "{\n",
      "  \"path\": \"/Users/olejacobmellgren/Documents/football_analysis/football_yolo_workdir/merged_yolo\",\n",
      "  \"train\": \"/Users/olejacobmellgren/Documents/football_analysis/football_yolo_workdir/merged_yolo/images/train\",\n",
      "  \"val\": \"/Users/olejacobmellgren/Documents/football_analysis/football_yolo_workdir/merged_yolo/images/val\",\n",
      "  \"names\": {\n",
      "    \"0\": \"player\",\n",
      "    \"1\": \"referee\",\n",
      "    \"2\": \"ball\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, yaml\n",
    "\n",
    "def _expected_yaml():\n",
    "    return {\n",
    "        'path': MERGED_ROOT.as_posix(),\n",
    "        'train': (MERGED_ROOT / 'images' / 'train').as_posix(),\n",
    "        'val':   (MERGED_ROOT / 'images' / 'val').as_posix(),\n",
    "        'names': {0:'player', 1:'referee', 2:'ball'}\n",
    "    }\n",
    "\n",
    "if DATA_YAML_PATH.exists():\n",
    "    with open(DATA_YAML_PATH) as f:\n",
    "        data_yaml = yaml.safe_load(f) or {}\n",
    "    expected = _expected_yaml()\n",
    "    needs_write = False\n",
    "    for key, value in expected.items():\n",
    "        if data_yaml.get(key) != value:\n",
    "            data_yaml[key] = value\n",
    "            needs_write = True\n",
    "    if needs_write:\n",
    "        with open(DATA_YAML_PATH, 'w') as f:\n",
    "            yaml.safe_dump(data_yaml, f, sort_keys=False)\n",
    "        print(\"Updated data.yaml with local paths:\", DATA_YAML_PATH)\n",
    "    else:\n",
    "        print(\"Loaded existing data.yaml:\", DATA_YAML_PATH)\n",
    "else:\n",
    "    data_yaml = _expected_yaml()\n",
    "    with open(DATA_YAML_PATH, 'w') as f:\n",
    "        yaml.safe_dump(data_yaml, f, sort_keys=False)\n",
    "    print(\"Wrote\", DATA_YAML_PATH)\n",
    "\n",
    "print(json.dumps(data_yaml, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39231f09",
   "metadata": {},
   "source": [
    "## 4) Quick stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0687fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] paired images: 10640 | boxes by class: {0: 239341, 1: 9452, 2: 68}\n",
      "[val] paired images: 1721 | boxes by class: {0: 38695, 1: 1521, 2: 12}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def class_counts_for_existing_pairs(root: Path, split=\"train\"):\n",
    "    img_dir = root / \"images\" / split\n",
    "    lbl_dir = root / \"labels\" / split\n",
    "    counts = Counter()\n",
    "    n_pairs = 0\n",
    "    # count only images with a matching label .txt\n",
    "    for img in img_dir.rglob(\"*.*\"):\n",
    "        if img.suffix.lower() not in {\".png\",\".jpg\",\".jpeg\"}:\n",
    "            continue\n",
    "        rel = img.relative_to(img_dir)\n",
    "        lbl = lbl_dir / rel.with_suffix(\".txt\")\n",
    "        if not lbl.exists():\n",
    "            continue\n",
    "        n_pairs += 1\n",
    "        with open(lbl) as f:\n",
    "            for line in f:\n",
    "                s = line.strip()\n",
    "                if not s: \n",
    "                    continue\n",
    "                cid = int(float(s.split()[0]))\n",
    "                counts[cid]+=1\n",
    "    print(f\"[{split}] paired images: {n_pairs} | boxes by class:\", dict(counts))\n",
    "\n",
    "class_counts_for_existing_pairs(MERGED_ROOT, \"train\")\n",
    "class_counts_for_existing_pairs(MERGED_ROOT, \"val\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cc197",
   "metadata": {},
   "source": [
    "## 5) Train YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfcb423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "371538a0",
   "metadata": {},
   "source": [
    "## 6) Evaluate (Precision, Recall, mAP@50, mAP@[0.5:0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4792e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faaa9e7",
   "metadata": {},
   "source": [
    "\n",
    "## Notes\n",
    "- We used **symlinks** to avoid duplicating data. If your environment disallows symlinks, switch `symlink(...)` to copy files instead.\n",
    "- We performed a global **90/10 train/val split**. If you prefer per-match splits or using the existing `train.txt` files, we can adapt easily.\n",
    "- The labels are assumed to already be in **YOLO detection format** with classes `[0=player, 1=referee, 2=ball]`. If not, we can parse `annotations.xml` (CVAT) and convert explicitly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Football (YOLO)",
   "language": "python",
   "name": "football_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
